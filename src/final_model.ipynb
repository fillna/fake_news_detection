{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from transformers import (BertForSequenceClassification, BertTokenizer,\n",
    "                          RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                          XLNetForSequenceClassification, XLNetTokenizer,\n",
    "                          AlbertForSequenceClassification, AlbertTokenizer,\n",
    "                          AdamW, get_linear_schedule_with_warmup\n",
    "                          )\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def encode_label(label:str):\n",
    "    if label == 'true': return 0\n",
    "    if label == 'mostly-true': return 1\n",
    "    if label == 'barely-true': return 2\n",
    "    if label == 'half-true': return 3\n",
    "    if label == 'false': return 4\n",
    "    if label == 'pants-fire': return 5\n",
    "    return -1\n",
    "\n",
    "def tokenize(X, y):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for txt in X.tolist():\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "                            txt,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = 100,\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt'\n",
    "                    )\n",
    "        input_ids.append(encoded_text['input_ids'])\n",
    "        attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(torch.from_numpy(y.to_numpy()))\n",
    "    \n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "def accuracy(pred, actual):\n",
    "    pred_flat = np.argmax(pred, axis=1).flatten()\n",
    "    labels_flat = actual.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>speaker</th>\n",
       "      <th>documented_time</th>\n",
       "      <th>author_score</th>\n",
       "      <th>headline</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>src_label</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>stated on October 28, 2023 in a screenshot sha...</td>\n",
       "      <td>4</td>\n",
       "      <td>Madison Czopek</td>\n",
       "      <td>October 31, 2023</td>\n",
       "      <td>[  5   3  16  54 473 152]</td>\n",
       "      <td>haaretz investig reveal discrep israel report ...</td>\n",
       "      <td>viral oct social medium post claim israel lie ...</td>\n",
       "      <td>haaretz isra newspap said x claim report blata...</td>\n",
       "      <td>4</td>\n",
       "      <td>81_gaza_palestinian_israelpalestin_israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>stated on May 30, 2023 in Interview:</td>\n",
       "      <td>2</td>\n",
       "      <td>Laura Schulte</td>\n",
       "      <td>October 31, 2023</td>\n",
       "      <td>[26 45 39 41 44 11]</td>\n",
       "      <td>wisconsin histor think larg continu blue state</td>\n",
       "      <td>wisconsin help swing presidenti vote donald tr...</td>\n",
       "      <td>although wisconsin vote democrat presidenti ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>3_wisconsin_governor_walker_republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>stated on October 27, 2023 in a post:</td>\n",
       "      <td>4</td>\n",
       "      <td>Ciara O'Rourke</td>\n",
       "      <td>October 30, 2023</td>\n",
       "      <td>[  5   3  16  54 473 152]</td>\n",
       "      <td>airport salzburg austria counter peopl flew au...</td>\n",
       "      <td>social medium post poi encourag peopl unfortun...</td>\n",
       "      <td>social medium post poi encourag peopl unfortun...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1_barack_obama_clinton_democrat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source                                            context  target  \\\n",
       "0  Instagram posts  stated on October 28, 2023 in a screenshot sha...       4   \n",
       "1     Scott Walker               stated on May 30, 2023 in Interview:       2   \n",
       "2  Instagram posts              stated on October 27, 2023 in a post:       4   \n",
       "\n",
       "          speaker    documented_time               author_score  \\\n",
       "0  Madison Czopek   October 31, 2023  [  5   3  16  54 473 152]   \n",
       "1   Laura Schulte   October 31, 2023        [26 45 39 41 44 11]   \n",
       "2  Ciara O'Rourke   October 30, 2023  [  5   3  16  54 473 152]   \n",
       "\n",
       "                                            headline  \\\n",
       "0  haaretz investig reveal discrep israel report ...   \n",
       "1     wisconsin histor think larg continu blue state   \n",
       "2  airport salzburg austria counter peopl flew au...   \n",
       "\n",
       "                                             article  \\\n",
       "0  viral oct social medium post claim israel lie ...   \n",
       "1  wisconsin help swing presidenti vote donald tr...   \n",
       "2  social medium post poi encourag peopl unfortun...   \n",
       "\n",
       "                                             summary  src_label  \\\n",
       "0  haaretz isra newspap said x claim report blata...          4   \n",
       "1  although wisconsin vote democrat presidenti ca...          1   \n",
       "2  social medium post poi encourag peopl unfortun...          4   \n",
       "\n",
       "                                       topic  \n",
       "0  81_gaza_palestinian_israelpalestin_israel  \n",
       "1     3_wisconsin_governor_walker_republican  \n",
       "2           -1_barack_obama_clinton_democrat  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/enriched_politifact.csv')\n",
    "df = df.dropna(subset='speaker').reset_index().drop(columns='index')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X, y, shuffle_idx, num_cols = 1):\n",
    "    cols = ['headline', 'source', 'speaker', 'context', 'topic', 'summary']\n",
    "\n",
    "    if(num_cols < 0 or num_cols > len(cols)):\n",
    "        print(\"Invalid number of columns. [1-6]\")\n",
    "        return\n",
    "    \n",
    "    X_subset = ''\n",
    "    for i in range(num_cols):\n",
    "        X_subset += X[cols[i]] + ' '\n",
    "\n",
    "    train_idx = shuffle_idx[:int(len(y) * 0.64)]\n",
    "    valid_idx = shuffle_idx[int(len(y) * 0.64)+1 : int(len(y) * 0.8)]\n",
    "    test_idx = shuffle_idx[int(len(y) * 0.8)+1:]\n",
    "\n",
    "    return X_subset[train_idx], X_subset[valid_idx], X_subset[test_idx], y[train_idx], y[valid_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "X, y = df, df['target']\n",
    "idx = list(range(df.shape[0]))\n",
    "random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TFIDF & Traditional ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.78\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.76\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.74\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.73\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 1 is: [{'epoch': 1, 'Training Loss': 1.7847257475535074, 'Valid. Loss': 1.740508392167244, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 2, 'Training Loss': 1.7598350903193156, 'Valid. Loss': 1.741771245816115, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 3, 'Training Loss': 1.744027058728536, 'Valid. Loss': 1.7313743501838081, 'Valid. Accur.': 0.2990405117270789}] ========\n",
      "F1 score: 0.0764, Accuracy: 0.297, Precision: 0.0496, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[   0    0    0    0  517    0]\n",
      " [   0    0    0    0  679    0]\n",
      " [   0    0    0    0  724    0]\n",
      " [   0    0    0    0  757    0]\n",
      " [   0    0    0    0 1393    0]\n",
      " [   0    0    0    0  615    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.79\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.76\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.74\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.73\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 2 is: [{'epoch': 1, 'Training Loss': 1.7852687650044758, 'Valid. Loss': 1.7404871861309386, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 2, 'Training Loss': 1.7599436747868855, 'Valid. Loss': 1.7421754242768928, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 3, 'Training Loss': 1.7449733110427856, 'Valid. Loss': 1.7320966494363, 'Valid. Accur.': 0.2990405117270789}] ========\n",
      "F1 score: 0.0764, Accuracy: 0.297, Precision: 0.0496, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[   0    0    0    0  517    0]\n",
      " [   0    0    0    0  679    0]\n",
      " [   0    0    0    0  724    0]\n",
      " [   0    0    0    0  757    0]\n",
      " [   0    0    0    0 1393    0]\n",
      " [   0    0    0    0  615    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.79\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.76\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.75\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.73\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 3 is: [{'epoch': 1, 'Training Loss': 1.7858165980656941, 'Valid. Loss': 1.7402617847487363, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 2, 'Training Loss': 1.7608380105336507, 'Valid. Loss': 1.7434354153777492, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 3, 'Training Loss': 1.7468412614186606, 'Valid. Loss': 1.7315497505131052, 'Valid. Accur.': 0.2990405117270789}] ========\n",
      "F1 score: 0.0764, Accuracy: 0.297, Precision: 0.0496, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[   0    0    0    0  517    0]\n",
      " [   0    0    0    0  679    0]\n",
      " [   0    0    0    0  724    0]\n",
      " [   0    0    0    0  757    0]\n",
      " [   0    0    0    0 1393    0]\n",
      " [   0    0    0    0  615    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.79\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.76\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.75\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.73\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 4 is: [{'epoch': 1, 'Training Loss': 1.7855671171188354, 'Valid. Loss': 1.7394638869808172, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 2, 'Training Loss': 1.761478601582845, 'Valid. Loss': 1.743528849280465, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 3, 'Training Loss': 1.7482638015111287, 'Valid. Loss': 1.7316504269520612, 'Valid. Accur.': 0.2990405117270789}] ========\n",
      "F1 score: 0.0764, Accuracy: 0.297, Precision: 0.0496, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[   0    0    0    0  517    0]\n",
      " [   0    0    0    0  679    0]\n",
      " [   0    0    0    0  724    0]\n",
      " [   0    0    0    0  757    0]\n",
      " [   0    0    0    0 1393    0]\n",
      " [   0    0    0    0  615    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.79\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.76\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.75\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.73\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 5 is: [{'epoch': 1, 'Training Loss': 1.7854980374654135, 'Valid. Loss': 1.7409092078585107, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 2, 'Training Loss': 1.7604555293401083, 'Valid. Loss': 1.7426364124456704, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 3, 'Training Loss': 1.7458740366617838, 'Valid. Loss': 1.7314910685329803, 'Valid. Accur.': 0.2990405117270789}] ========\n",
      "F1 score: 0.0764, Accuracy: 0.297, Precision: 0.0496, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[   0    0    0    0  517    0]\n",
      " [   0    0    0    0  679    0]\n",
      " [   0    0    0    0  724    0]\n",
      " [   0    0    0    0  757    0]\n",
      " [   0    0    0    0 1393    0]\n",
      " [   0    0    0    0  615    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.79\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.76\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.74\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1875.\n",
      "Avg. Train Loss: 1.75\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.30\n",
      "Val Loss: 1.73\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 6 is: [{'epoch': 1, 'Training Loss': 1.785743014717102, 'Valid. Loss': 1.740984964218221, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 2, 'Training Loss': 1.7606256395975748, 'Valid. Loss': 1.7423761579781962, 'Valid. Accur.': 0.2990405117270789}, {'epoch': 3, 'Training Loss': 1.7464404373804727, 'Valid. Loss': 1.7314699583216262, 'Valid. Accur.': 0.2990405117270789}] ========\n",
      "F1 score: 0.0764, Accuracy: 0.297, Precision: 0.0496, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[   0    0    0    0  517    0]\n",
      " [   0    0    0    0  679    0]\n",
      " [   0    0    0    0  724    0]\n",
      " [   0    0    0    0  757    0]\n",
      " [   0    0    0    0 1393    0]\n",
      " [   0    0    0    0  615    0]]\n"
     ]
    }
   ],
   "source": [
    "# Imbalanced Class Distribution\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# Loop through 6 columns\n",
    "for i in range(1, 7):\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = data_split(X, y, idx, num_cols=i)\n",
    "    train_input_ids, train_attention_masks, train_labels = tokenize(X_train, y_train)\n",
    "    valid_input_ids, valid_attention_masks, valid_labels = tokenize(X_valid, y_valid)\n",
    "    test_input_ids, test_attention_masks, test_labels = tokenize(X_test, y_test)\n",
    "\n",
    "    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "    epochs = 3\n",
    "    batch_size = 8\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset),\n",
    "                batch_size = batch_size)\n",
    "    valid_dataloader = DataLoader(\n",
    "                valid_dataset,\n",
    "                sampler = SequentialSampler(valid_dataset),\n",
    "                batch_size = batch_size)\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset,\n",
    "                sampler = SequentialSampler(test_dataset),\n",
    "                batch_size = batch_size)\n",
    "    \n",
    "    # Model\n",
    "    model = XLNetForSequenceClassification.from_pretrained(\n",
    "    \"xlnet-base-cased\",\n",
    "    num_labels = 6, \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False)\n",
    "\n",
    "    desc = model.cuda()\n",
    "    optimizer = AdamW(model.parameters(), lr = 2e-4, eps = 1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0,\n",
    "                                                num_training_steps = len(train_dataloader) * epochs)\n",
    "    \n",
    "\n",
    "    # Train\n",
    "    seed_val = 30\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            if step % 1000 == 0 and not step == 0:\n",
    "                print('Batch {} of {}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_ids, b_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()        \n",
    "            loss, logits = model(b_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_mask, \n",
    "                                labels=b_labels,\n",
    "                                return_dict=False)\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "                \n",
    "        print(\"Avg. Train Loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"=====Validating====\")\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        for batch in valid_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_ids, b_mask, b_labels = batch\n",
    "            \n",
    "            with torch.no_grad():        \n",
    "                (loss, logits, _) = model(b_ids, \n",
    "                                    token_type_ids=None, \n",
    "                                    attention_mask=b_mask,\n",
    "                                    labels=b_labels,\n",
    "                                    return_dict=False)\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            total_eval_accuracy += accuracy(logits, label_ids)\n",
    "            \n",
    "        avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n",
    "        print(\"Avg. Val Acc: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(valid_dataloader)\n",
    "        print(\"Val Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print('======== Training Stat for i = {} is: {} ========'.format(i, training_stats))\n",
    "\n",
    "\n",
    "    # Eval\n",
    "    pred_probas , labels = [], []\n",
    "\n",
    "    for (step, batch) in enumerate(test_dataloader):\n",
    "    \n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            print('Batch {} of {}.'.format(step, len(test_dataloader)))\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_ids, b_mask, b_labels = batch\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_ids, token_type_ids=None, attention_mask=b_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "        pred_probas.append(logits)\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    preds = np.concatenate(pred_probas, axis=0)\n",
    "    ytrue = np.concatenate(labels, axis=0)\n",
    "    ypred = np.argmax(preds, axis=1)\n",
    "\n",
    "    f1 = metrics.f1_score(ytrue, ypred, average='macro')\n",
    "    acc = metrics.accuracy_score(ytrue, ypred)\n",
    "    precision = metrics.precision_score(ytrue, ypred, average='macro')\n",
    "    recall = metrics.recall_score(ytrue, ypred, average='macro')\n",
    "    cmatrix = metrics.confusion_matrix(ytrue, ypred)\n",
    "    print('F1 score: {:.3}, Accuracy: {:.3}, Precision: {:.3}, Recall: {:.3}, \\n Confusion Matrix: \\n {}'.format(f1, acc, precision, recall, cmatrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.84\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.81\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.81\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.79\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.80\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.15\n",
      "Val Loss: 1.79\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 1 is: [{'epoch': 1, 'Training Loss': 1.844217666277445, 'Valid. Loss': 1.8073982676634421, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 2, 'Training Loss': 1.814271642788347, 'Valid. Loss': 1.7920895516872406, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 3, 'Training Loss': 1.8018984930582314, 'Valid. Loss': 1.7942728732640927, 'Valid. Accur.': 0.14623397435897437}] ========\n",
      "F1 score: 0.0473, Accuracy: 0.165, Precision: 0.0276, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[  0 534   0   0   0   0]\n",
      " [  0 515   0   0   0   0]\n",
      " [  0 508   0   0   0   0]\n",
      " [  0 485   0   0   0   0]\n",
      " [  0 519   0   0   0   0]\n",
      " [  0 551   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.84\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.81\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.82\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.16\n",
      "Val Loss: 1.79\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.80\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.79\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 2 is: [{'epoch': 1, 'Training Loss': 1.8431228709508138, 'Valid. Loss': 1.8056501623911736, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 2, 'Training Loss': 1.8166435224464141, 'Valid. Loss': 1.791951359846653, 'Valid. Accur.': 0.16466346153846154}, {'epoch': 3, 'Training Loss': 1.8038822718892231, 'Valid. Loss': 1.794546332114782, 'Valid. Accur.': 0.17708333333333334}] ========\n",
      "F1 score: 0.0449, Accuracy: 0.156, Precision: 0.026, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[  0   0   0 534   0   0]\n",
      " [  0   0   0 515   0   0]\n",
      " [  0   0   0 508   0   0]\n",
      " [  0   0   0 485   0   0]\n",
      " [  0   0   0 519   0   0]\n",
      " [  0   0   0 551   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.84\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.80\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.82\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.16\n",
      "Val Loss: 1.79\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.81\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.15\n",
      "Val Loss: 1.80\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 3 is: [{'epoch': 1, 'Training Loss': 1.843422971288842, 'Valid. Loss': 1.804953130391928, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 2, 'Training Loss': 1.818295667544905, 'Valid. Loss': 1.7913924661966472, 'Valid. Accur.': 0.16466346153846154}, {'epoch': 3, 'Training Loss': 1.8062816424542163, 'Valid. Loss': 1.7951595133695848, 'Valid. Accur.': 0.14623397435897437}] ========\n",
      "F1 score: 0.0473, Accuracy: 0.165, Precision: 0.0276, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[  0 534   0   0   0   0]\n",
      " [  0 515   0   0   0   0]\n",
      " [  0 508   0   0   0   0]\n",
      " [  0 485   0   0   0   0]\n",
      " [  0 519   0   0   0   0]\n",
      " [  0 551   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.84\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.81\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.82\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.16\n",
      "Val Loss: 1.79\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.81\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.17\n",
      "Val Loss: 1.79\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 4 is: [{'epoch': 1, 'Training Loss': 1.8441005306550298, 'Valid. Loss': 1.8059642586188438, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 2, 'Training Loss': 1.8187992663747337, 'Valid. Loss': 1.7912747791180244, 'Valid. Accur.': 0.16466346153846154}, {'epoch': 3, 'Training Loss': 1.8068362263790574, 'Valid. Loss': 1.7948151662563667, 'Valid. Accur.': 0.1658653846153846}] ========\n",
      "F1 score: 0.0468, Accuracy: 0.163, Precision: 0.0272, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[  0   0 534   0   0   0]\n",
      " [  0   0 515   0   0   0]\n",
      " [  0   0 508   0   0   0]\n",
      " [  0   0 485   0   0   0]\n",
      " [  0   0 519   0   0   0]\n",
      " [  0   0 551   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.85\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.81\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.82\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.79\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.81\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.79\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 5 is: [{'epoch': 1, 'Training Loss': 1.8452703753628406, 'Valid. Loss': 1.8070262101216195, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 2, 'Training Loss': 1.8193726408433724, 'Valid. Loss': 1.7908107390006383, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 3, 'Training Loss': 1.806865449794325, 'Valid. Loss': 1.794233045134789, 'Valid. Accur.': 0.17708333333333334}] ========\n",
      "F1 score: 0.0449, Accuracy: 0.156, Precision: 0.026, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[  0   0   0 534   0   0]\n",
      " [  0   0   0 515   0   0]\n",
      " [  0   0   0 508   0   0]\n",
      " [  0   0   0 485   0   0]\n",
      " [  0   0   0 519   0   0]\n",
      " [  0   0   0 551   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.84\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.81\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.82\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.18\n",
      "Val Loss: 1.79\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 1000 of 1245.\n",
      "Avg. Train Loss: 1.81\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.15\n",
      "Val Loss: 1.79\n",
      "\n",
      "Training complete!\n",
      "======== Training Stat for i = 6 is: [{'epoch': 1, 'Training Loss': 1.8437713273557794, 'Valid. Loss': 1.8053208203651967, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 2, 'Training Loss': 1.8199051489312965, 'Valid. Loss': 1.7909569159532204, 'Valid. Accur.': 0.17708333333333334}, {'epoch': 3, 'Training Loss': 1.8077350846256117, 'Valid. Loss': 1.7945181352969928, 'Valid. Accur.': 0.14623397435897437}] ========\n",
      "F1 score: 0.0473, Accuracy: 0.165, Precision: 0.0276, Recall: 0.167, \n",
      " Confusion Matrix: \n",
      " [[  0 534   0   0   0   0]\n",
      " [  0 515   0   0   0   0]\n",
      " [  0 508   0   0   0   0]\n",
      " [  0 485   0   0   0   0]\n",
      " [  0 519   0   0   0   0]\n",
      " [  0 551   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Balanced Class Distribution\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# Loop through 6 columns\n",
    "for i in range(1, 7):\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = data_split(X, y, idx, num_cols=i)\n",
    "    train_input_ids, train_attention_masks, train_labels = tokenize(X_train, y_train)\n",
    "    valid_input_ids, valid_attention_masks, valid_labels = tokenize(X_valid, y_valid)\n",
    "    test_input_ids, test_attention_masks, test_labels = tokenize(X_test, y_test)\n",
    "\n",
    "    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    valid_dataset = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "    epochs = 3\n",
    "    batch_size = 8\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset),\n",
    "                batch_size = batch_size)\n",
    "    valid_dataloader = DataLoader(\n",
    "                valid_dataset,\n",
    "                sampler = SequentialSampler(valid_dataset),\n",
    "                batch_size = batch_size)\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset,\n",
    "                sampler = SequentialSampler(test_dataset),\n",
    "                batch_size = batch_size)\n",
    "    \n",
    "    # Model\n",
    "    model = XLNetForSequenceClassification.from_pretrained(\n",
    "    \"xlnet-base-cased\",\n",
    "    num_labels = 6, \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False)\n",
    "\n",
    "    desc = model.cuda()\n",
    "    optimizer = AdamW(model.parameters(), lr = 2e-4, eps = 1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0,\n",
    "                                                num_training_steps = len(train_dataloader) * epochs)\n",
    "    \n",
    "\n",
    "    # Train\n",
    "    seed_val = 30\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            if step % 1000 == 0 and not step == 0:\n",
    "                print('Batch {} of {}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_ids, b_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()        \n",
    "            loss, logits = model(b_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_mask, \n",
    "                                labels=b_labels,\n",
    "                                return_dict=False)\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "                \n",
    "        print(\"Avg. Train Loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"=====Validating====\")\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        for batch in valid_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_ids, b_mask, b_labels = batch\n",
    "            \n",
    "            with torch.no_grad():        \n",
    "                (loss, logits, _) = model(b_ids, \n",
    "                                    token_type_ids=None, \n",
    "                                    attention_mask=b_mask,\n",
    "                                    labels=b_labels,\n",
    "                                    return_dict=False)\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            total_eval_accuracy += accuracy(logits, label_ids)\n",
    "            \n",
    "        avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n",
    "        print(\"Avg. Val Acc: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(valid_dataloader)\n",
    "        print(\"Val Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print('======== Training Stat for i = {} is: {} ========'.format(i, training_stats))\n",
    "\n",
    "\n",
    "    # Eval\n",
    "    pred_probas , labels = [], []\n",
    "\n",
    "    for (step, batch) in enumerate(test_dataloader):\n",
    "    \n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            print('Batch {} of {}.'.format(step, len(test_dataloader)))\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_ids, b_mask, b_labels = batch\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_ids, token_type_ids=None, attention_mask=b_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "        pred_probas.append(logits)\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    preds = np.concatenate(pred_probas, axis=0)\n",
    "    ytrue = np.concatenate(labels, axis=0)\n",
    "    ypred = np.argmax(preds, axis=1)\n",
    "\n",
    "    f1 = metrics.f1_score(ytrue, ypred, average='macro')\n",
    "    acc = metrics.accuracy_score(ytrue, ypred)\n",
    "    precision = metrics.precision_score(ytrue, ypred, average='macro')\n",
    "    recall = metrics.recall_score(ytrue, ypred, average='macro')\n",
    "    cmatrix = metrics.confusion_matrix(ytrue, ypred)\n",
    "    print('F1 score: {:.3}, Accuracy: {:.3}, Precision: {:.3}, Recall: {:.3}, \\n Confusion Matrix: \\n {}'.format(f1, acc, precision, recall, cmatrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    "valid_dataloader = DataLoader(\n",
    "            valid_dataset,\n",
    "            sampler = SequentialSampler(valid_dataset),\n",
    "            batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    \"xlnet-base-cased\",\n",
    "    num_labels = 6, \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False)\n",
    "\n",
    "desc = model.cuda()\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-4, eps = 1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = len(train_dataloader) * epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 30\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            print('Batch {} of {}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_ids, b_mask, b_labels = batch\n",
    "\n",
    "        model.zero_grad()        \n",
    "        loss, logits = model(b_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_mask, \n",
    "                             labels=b_labels,\n",
    "                             return_dict=False)\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "               \n",
    "    print(\"Avg. Train Loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"=====Validating====\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_ids, b_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            (loss, logits, _) = model(b_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_mask,\n",
    "                                   labels=b_labels,\n",
    "                                   return_dict=False)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n",
    "    print(\"Avg. Val Acc: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(valid_dataloader)\n",
    "    print(\"Val Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas , labels = [], []\n",
    "\n",
    "for (step, batch) in enumerate(valid_dataloader):\n",
    "  \n",
    "    if step % 500 == 0 and not step == 0:\n",
    "        print('Batch {} of {}.'.format(step, len(valid_dataloader)))\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_ids, b_mask, b_labels = batch\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_ids, token_type_ids=None, attention_mask=b_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    pred_probas.append(logits)\n",
    "    labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(pred_probas, axis=0)\n",
    "ytrue = np.concatenate(labels, axis=0)\n",
    "ypred = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "f1 = metrics.f1_score(ytrue, ypred, average='macro')\n",
    "acc = metrics.accuracy_score(ytrue, ypred)\n",
    "precision = metrics.precision_score(ytrue, ypred, average='macro')\n",
    "recall = metrics.recall_score(ytrue, ypred, average='macro')\n",
    "cmatrix = metrics.confusion_matrix(ytrue, ypred)\n",
    "print('F1 score: {:.3}, Accuracy: {:.3}, Precision: {:.3}, Recall: {:.3}, \\n Confusion Matrix: \\n {}'.format(f1, acc, precision, recall, cmatrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clickbait import ClickbaitModel\n",
    "from sentiment_log import SentimentModel\n",
    "from spam import SpamModel\n",
    "from source_reliable import SourceReliableModel\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['target'], test_size=.2, random_state=11)\n",
    "\n",
    "clickM = ClickbaitModel()\n",
    "sentiM = SentimentModel()\n",
    "spamM = SpamModel()\n",
    "srcM = SourceReliableModel()\n",
    "\n",
    "sentiM.fit(X_train)\n",
    "spamM.fit(X_train)\n",
    "srcM.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lst = [0.946, 0.976, 0.412, 0.589]\n",
    "weight = [acc/sum(acc_lst) for acc in acc_lst]\n",
    "clickbaitV = clickM.predict(X_test)[1] * weight[0]\n",
    "sentiV = sentiM.predict(X_test)[1] * weight[1]\n",
    "spamV = spamM.predict(X_test)[1] * weight[2]\n",
    "sourceV = srcM.predict(X_test)[1] * weight[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clickbaitV = clickM.predict(X_train)[1] * weight[0]\n",
    "train_sentiV = sentiM.predict(X_train)[1] * weight[1]\n",
    "train_spamV = spamM.predict(X_train)[1] * weight[2]\n",
    "train_sourceV = srcM.predict(X_train)[1] * weight[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veracity_train = pd.DataFrame({'clickbait':train_clickbaitV, 'sentiment': train_sentiV, 'spam': train_spamV, 'source': train_sourceV})\n",
    "veracity_test = pd.DataFrame({'clickbait':clickbaitV, 'sentiment': sentiV, 'spam': spamV, 'source': sourceV})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(veracity_train, y_train)\n",
    "clf.score(veracity_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
