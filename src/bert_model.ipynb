{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (BertForSequenceClassification, BertTokenizer,\n",
    "                          RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                          XLMForSequenceClassification, XLMTokenizer,\n",
    "                          XLNetForSequenceClassification, XLNetTokenizer,\n",
    "                          DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          AlbertForSequenceClassification, AlbertTokenizer,\n",
    "                          AdamW, get_linear_schedule_with_warmup\n",
    "                          )\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label:str):\n",
    "    if label == 'true': return 0\n",
    "    if label == 'mostly-true': return 1\n",
    "    if label == 'barely-true': return 2\n",
    "    if label == 'half-true': return 3\n",
    "    if label == 'false': return 4\n",
    "    if label == 'pants-fire': return 5\n",
    "    return -1\n",
    "\n",
    "def load_df(liar_path:str):\n",
    "    df = pd.read_csv(liar_path, sep='\\t', header=None, quoting=csv.QUOTE_NONE, usecols=[2,3,5,14,15]).dropna()\n",
    "    df = df.rename(columns={2:'target'})    \n",
    "    df['text'] = df[3] + '. ' + df[5] + '. ' + df[14] + '. ' + df[15]\n",
    "    df['target'] = df['target'].apply(encode_label)   \n",
    "    return df[['text', 'target']]\n",
    "\n",
    "def tokenize(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for txt in df['text'].tolist():\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "                            txt,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = 400,\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt'\n",
    "                    )\n",
    "        input_ids.append(encoded_text['input_ids'])\n",
    "        attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    labels = torch.tensor(torch.from_numpy(df['target'].to_numpy()))\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "def accuracy(pred, actual):\n",
    "    pred_flat = np.argmax(pred, axis=1).flatten()\n",
    "    labels_flat = actual.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "train_df = load_df('../data/liar_plus/train2.tsv')\n",
    "test_df = load_df('../data/liar_plus/test2.tsv')\n",
    "valid_df = load_df('../data/liar_plus/val2.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Jun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jun\\AppData\\Local\\Temp\\ipykernel_24280\\1228417583.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(torch.from_numpy(df['target'].to_numpy()))\n"
     ]
    }
   ],
   "source": [
    "train_input_ids, train_attention_masks, train_labels = tokenize(train_df)\n",
    "valid_input_ids, valid_attention_masks, valid_labels = tokenize(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    "valid_dataloader = DataLoader(\n",
    "            valid_dataset,\n",
    "            sampler = SequentialSampler(valid_dataset),\n",
    "            batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Jun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\", \n",
    "#     num_labels = 6, \n",
    "#     output_attentions = False,\n",
    "#     output_hidden_states = False\n",
    "# )\n",
    "\n",
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    \"xlnet-base-cased\",\n",
    "    num_labels = 6, \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False)\n",
    "\n",
    "desc = model.cuda()\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = len(train_dataloader) * epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert-base-uncaed:\n",
    "- seed = 30\n",
    "- lr = 2e-5, eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Batch 100 of 1257.\n",
      "Batch 200 of 1257.\n",
      "Batch 300 of 1257.\n",
      "Batch 400 of 1257.\n",
      "Batch 500 of 1257.\n",
      "Batch 600 of 1257.\n",
      "Batch 700 of 1257.\n",
      "Batch 800 of 1257.\n",
      "Batch 900 of 1257.\n",
      "Batch 1000 of 1257.\n",
      "Batch 1100 of 1257.\n",
      "Batch 1200 of 1257.\n",
      "Avg. Train Loss: 1.78\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.19\n",
      "Val Loss: 1.76\n",
      "======== Epoch 2 / 3 ========\n",
      "Batch 100 of 1257.\n",
      "Batch 200 of 1257.\n",
      "Batch 300 of 1257.\n",
      "Batch 400 of 1257.\n",
      "Batch 500 of 1257.\n",
      "Batch 600 of 1257.\n",
      "Batch 700 of 1257.\n",
      "Batch 800 of 1257.\n",
      "Batch 900 of 1257.\n",
      "Batch 1000 of 1257.\n",
      "Batch 1100 of 1257.\n",
      "Batch 1200 of 1257.\n",
      "Avg. Train Loss: 1.74\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.27\n",
      "Val Loss: 1.68\n",
      "======== Epoch 3 / 3 ========\n",
      "Batch 100 of 1257.\n",
      "Batch 200 of 1257.\n",
      "Batch 300 of 1257.\n",
      "Batch 400 of 1257.\n",
      "Batch 500 of 1257.\n",
      "Batch 600 of 1257.\n",
      "Batch 700 of 1257.\n",
      "Batch 800 of 1257.\n",
      "Batch 900 of 1257.\n",
      "Batch 1000 of 1257.\n",
      "Batch 1100 of 1257.\n",
      "Batch 1200 of 1257.\n",
      "Avg. Train Loss: 1.64\n",
      "=====Validating====\n",
      "Avg. Val Acc: 0.27\n",
      "Val Loss: 1.65\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 30\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('Batch {} of {}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_ids, b_mask, b_labels = batch\n",
    "\n",
    "        model.zero_grad()        \n",
    "        loss, logits = model(b_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_mask, \n",
    "                             labels=b_labels,\n",
    "                             return_dict=False)\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "               \n",
    "    print(\"Avg. Train Loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"=====Validating====\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_ids, b_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            (loss, logits, _) = model(b_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_mask,\n",
    "                                   labels=b_labels,\n",
    "                                   return_dict=False)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n",
    "    print(\"Avg. Val Acc: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(valid_dataloader)\n",
    "    print(\"Val Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\AppData\\Local\\Temp\\ipykernel_24280\\1228417583.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(torch.from_numpy(df['target'].to_numpy()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50 of 156.\n",
      "Batch 100 of 156.\n",
      "Batch 150 of 156.\n"
     ]
    }
   ],
   "source": [
    "test_input_ids, test_attention_masks, test_labels = tokenize(test_df)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = RandomSampler(test_dataset),\n",
    "            batch_size = batch_size)\n",
    "model.eval()\n",
    "pred_probas , labels = [], []\n",
    "\n",
    "for (step, batch) in enumerate(test_dataloader):\n",
    "  \n",
    "    if step % 50 == 0 and not step == 0:\n",
    "        print('Batch {} of {}.'.format(step, len(test_dataloader)))\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_ids, b_mask, b_labels = batch\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_ids, token_type_ids=None, attention_mask=b_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    pred_probas.append(logits)\n",
    "    labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(pred_probas, axis=0)\n",
    "ytrue = np.concatenate(labels, axis=0)\n",
    "ypred = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.285, Accuracy: 0.295, Precision: 0.314, Recall: 0.287, \n",
      " Confusion Matrix: \n",
      " [[ 68  46   2  59  21   5]\n",
      " [ 53  76   5  75  27   1]\n",
      " [ 20  34  21  94  33   8]\n",
      " [ 30  57  14 120  37   1]\n",
      " [ 36  26  23  83  59  19]\n",
      " [ 11   5   6  23  21  23]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "f1 = metrics.f1_score(ytrue, ypred, average='macro')\n",
    "acc = metrics.accuracy_score(ytrue, ypred)\n",
    "precision = metrics.precision_score(ytrue, ypred, average='macro')\n",
    "recall = metrics.recall_score(ytrue, ypred, average='macro')\n",
    "cmatrix = metrics.confusion_matrix(ytrue, ypred)\n",
    "print('F1 score: {:.3}, Accuracy: {:.3}, Precision: {:.3}, Recall: {:.3}, \\n Confusion Matrix: \\n {}'.format(f1, acc, precision, recall, cmatrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
